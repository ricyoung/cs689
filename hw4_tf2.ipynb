{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4_tf2ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMMr3X9T06CJcXCksrVLWac",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ricyoung/cs689/blob/master/hw4_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F15NBYbMtnWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBskq6L5Kkne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import datetime, os\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9211TcMKkcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_dataset, mnist_info = tfds.load(name = 'mnist', with_info=True, as_supervised=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SQLmheRKkaB",
        "colab_type": "code",
        "outputId": "c4cd8b31-8598-4da8-f69b-60f318e8bd82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "print(mnist_info) "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='mnist',\n",
            "    version=3.0.0,\n",
            "    description='The MNIST database of handwritten digits.',\n",
            "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
            "    }),\n",
            "    total_num_examples=70000,\n",
            "    splits={\n",
            "        'test': 10000,\n",
            "        'train': 60000,\n",
            "    },\n",
            "    supervised_keys=('image', 'label'),\n",
            "    citation=\"\"\"@article{lecun2010mnist,\n",
            "      title={MNIST handwritten digit database},\n",
            "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
            "      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
            "      volume={2},\n",
            "      year={2010}\n",
            "    }\"\"\",\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNOv1Iq5KkUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encode for 10 MNIST classes\n",
        "def my_one_hot(feature, label):\n",
        "    return feature, tf.one_hot(label, depth=10)\n",
        "\n",
        "# # load your data from tfds\n",
        "# mnist_train, train_info = tfds.load(name=\"mnist\", with_info=True, as_supervised=True, split=tfds.Split.TRAIN)\n",
        "\n",
        "# # convert your labels in one-hot\n",
        "# mnist_train = mnist_train.map(my_one_hot)\n",
        "\n",
        "# you can batch your data here\n",
        "# mnist_train = mnist_train.batch(8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOp2029mpJA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batch_size=-1 to get the full dataset in NumPy arrays from the returned tf.Tensor object\n",
        "\n",
        "# mnist_train = tfds.load(name=\"mnist\", split=tfds.Split.TRAIN,as_supervised=True, batch_size=-1 ) \n",
        "# mnist_test = tfds.load(name=\"mnist\", split=tfds.Split.TEST,as_supervised=True, batch_size=-1)\n",
        "\n",
        "mnist_train = tfds.load(name=\"mnist\", split=tfds.Split.TRAIN, batch_size=-1 ) \n",
        "mnist_test = tfds.load(name=\"mnist\", split=tfds.Split.TEST, batch_size=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlG7fZ6hKkRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# tfds.as_numpy return a generator that yields NumPy array records out of a tf.data.Dataset\n",
        "mnist_train = tfds.as_numpy(mnist_train) \n",
        "mnist_test = tfds.as_numpy(mnist_test)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86y3rP3upGem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = mnist_train[\"image\"], mnist_train[\"label\"] # seperate the x and y\n",
        "x_test, y_test = mnist_test[\"image\"], mnist_test[\"label\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXcV_mmqSPBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.metrics import confusion_matrix, accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miwtO_RnSO9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# kf = KFold(n_splits=5, shuffle=True )\n",
        "# kf.get_n_splits(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sWorgRjSO6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(kf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEIeCg2LSOyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This doesn't work \n",
        "\n",
        "# for train_index, test_index in kf.split(x_train):\n",
        "#   print(\"TRAIN:\", train_index, \"TEST:\", test_index) \n",
        "  # X_train, X_test =x_train[train_index], x_train[test_index]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUa932AMMlLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#k-fold - split data index into 10parts\n",
        "batch = []\n",
        "for i in range (10):\n",
        "    batch.append([])\n",
        "    for j in range (len(x_train)):\n",
        "        if j%10 == i:\n",
        "            batch[i].append(x_train[j])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS8Eaj5PPIDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2YHSRNoKigL",
        "colab_type": "code",
        "outputId": "691df2bf-c6a4-4186-c113-d221bbb9bc4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# for train_index, test_index in kf.split(x_train):\n",
        "# Keep results for plotting\n",
        "train_loss_results = []\n",
        "train_accuracy_results = []\n",
        "\n",
        "for i in range(len(batch)):\n",
        "    print(i)\n",
        "\n",
        "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "    #reduces to 0-1 from 0-255\n",
        "    #this may not have much of a change on the output\n",
        "    x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
        "    x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
        "\n",
        "    #Using sequential model with 2 layered neural network with each having 128 neurons\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "        # model.add(tf.keras.layers.Dense(768, activation=tf.nn.relu))\n",
        "    # model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu))\n",
        "    model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
        "    model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
        "    model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n",
        "\n",
        "    model.compile(\n",
        "        \n",
        "                  optimizer= 'adam', #95% to 99%\n",
        "                  #optimizer= 'SGD', # so far not as good as adam\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'] )\n",
        "\n",
        "    \n",
        "    model.fit(x_train, y_train, epochs=6)\n",
        "    # epoch_loss_avg.update_state(val_loss)\n",
        "    # epoch_loss_avg.update_state(val_loss) \n",
        "\n",
        "    val_loss, val_acc = model.evaluate(x_test, y_test)\n",
        "    print('training lost:', train_loss_results)\n",
        "\n",
        "    # fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
        "    # fig.suptitle('Training Metrics')\n",
        "\n",
        "    # axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
        "    # axes[0].plot(train_loss_results)\n",
        "\n",
        "    # axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
        "    # axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
        "    # axes[1].plot(train_accuracy_results)\n",
        "    # plt.show()\n",
        "\n",
        "    print(val_acc, val_loss)\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Epoch 1/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2797 - accuracy: 0.9183\n",
            "Epoch 2/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1146 - accuracy: 0.9652\n",
            "Epoch 3/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0772 - accuracy: 0.9757\n",
            "Epoch 4/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0579 - accuracy: 0.9813\n",
            "Epoch 5/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0438 - accuracy: 0.9856\n",
            "Epoch 6/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0334 - accuracy: 0.9892\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.0955 - accuracy: 0.9731\n",
            "training lost: []\n",
            "0.9731000065803528 0.09546498209238052\n",
            "1\n",
            "Epoch 1/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2847 - accuracy: 0.9181\n",
            "Epoch 2/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1180 - accuracy: 0.9643\n",
            "Epoch 3/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0808 - accuracy: 0.9751\n",
            "Epoch 4/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0595 - accuracy: 0.9813\n",
            "Epoch 5/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0462 - accuracy: 0.9851\n",
            "Epoch 6/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0353 - accuracy: 0.9886\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1005 - accuracy: 0.9714\n",
            "training lost: []\n",
            "0.9714000225067139 0.10049675405025482\n",
            "2\n",
            "Epoch 1/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2693 - accuracy: 0.9221\n",
            "Epoch 2/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1112 - accuracy: 0.9656\n",
            "Epoch 3/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0739 - accuracy: 0.9771\n",
            "Epoch 4/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0541 - accuracy: 0.9824\n",
            "Epoch 5/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0425 - accuracy: 0.9862\n",
            "Epoch 6/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0307 - accuracy: 0.9898\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.0968 - accuracy: 0.9715\n",
            "training lost: []\n",
            "0.9714999794960022 0.09675003588199615\n",
            "3\n",
            "Epoch 1/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2802 - accuracy: 0.9195\n",
            "Epoch 2/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1172 - accuracy: 0.9642\n",
            "Epoch 3/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0788 - accuracy: 0.9752\n",
            "Epoch 4/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0582 - accuracy: 0.9818\n",
            "Epoch 5/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0435 - accuracy: 0.9856\n",
            "Epoch 6/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0338 - accuracy: 0.9889\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9726\n",
            "training lost: []\n",
            "0.972599983215332 0.09845650941133499\n",
            "4\n",
            "Epoch 1/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2762 - accuracy: 0.9203\n",
            "Epoch 2/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1120 - accuracy: 0.9657\n",
            "Epoch 3/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0763 - accuracy: 0.9758\n",
            "Epoch 4/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0562 - accuracy: 0.9826\n",
            "Epoch 5/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0422 - accuracy: 0.9868\n",
            "Epoch 6/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0333 - accuracy: 0.9892\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9716\n",
            "training lost: []\n",
            "0.9715999960899353 0.10187163203954697\n",
            "5\n",
            "Epoch 1/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2776 - accuracy: 0.9184\n",
            "Epoch 2/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1152 - accuracy: 0.9646\n",
            "Epoch 3/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0781 - accuracy: 0.9759\n",
            "Epoch 4/6\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0573 - accuracy: 0.9819\n",
            "Epoch 5/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0439 - accuracy: 0.9859\n",
            "Epoch 6/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0353 - accuracy: 0.9887\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.0936 - accuracy: 0.9737\n",
            "training lost: []\n",
            "0.9736999869346619 0.09363699704408646\n",
            "6\n",
            "Epoch 1/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2785 - accuracy: 0.9188\n",
            "Epoch 2/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1129 - accuracy: 0.9650\n",
            "Epoch 3/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0762 - accuracy: 0.9758\n",
            "Epoch 4/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0554 - accuracy: 0.9827\n",
            "Epoch 5/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0420 - accuracy: 0.9864\n",
            "Epoch 6/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0335 - accuracy: 0.9894\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.9759\n",
            "training lost: []\n",
            "0.9758999943733215 0.08575733006000519\n",
            "7\n",
            "Epoch 1/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2726 - accuracy: 0.9201\n",
            "Epoch 2/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1091 - accuracy: 0.9673\n",
            "Epoch 3/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0754 - accuracy: 0.9766\n",
            "Epoch 4/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0556 - accuracy: 0.9825\n",
            "Epoch 5/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0433 - accuracy: 0.9865\n",
            "Epoch 6/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0340 - accuracy: 0.9887\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9715\n",
            "training lost: []\n",
            "0.9714999794960022 0.10325337201356888\n",
            "8\n",
            "Epoch 1/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2732 - accuracy: 0.9208\n",
            "Epoch 2/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1138 - accuracy: 0.9649\n",
            "Epoch 3/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0779 - accuracy: 0.9755\n",
            "Epoch 4/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0587 - accuracy: 0.9810\n",
            "Epoch 5/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0444 - accuracy: 0.9859\n",
            "Epoch 6/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0331 - accuracy: 0.9894\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.0948 - accuracy: 0.9710\n",
            "training lost: []\n",
            "0.9710000157356262 0.09481114149093628\n",
            "9\n",
            "Epoch 1/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2724 - accuracy: 0.9206\n",
            "Epoch 2/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1144 - accuracy: 0.9659\n",
            "Epoch 3/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0781 - accuracy: 0.9754\n",
            "Epoch 4/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0588 - accuracy: 0.9813\n",
            "Epoch 5/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0433 - accuracy: 0.9864\n",
            "Epoch 6/6\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0340 - accuracy: 0.9890\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.0916 - accuracy: 0.9725\n",
            "training lost: []\n",
            "0.9725000262260437 0.09164644777774811\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVoXLIs3xocG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
        "# fig.suptitle('Training Metrics')\n",
        "\n",
        "# axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
        "# axes[0].plot(val_loss)\n",
        "\n",
        "# axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
        "# axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
        "# axes[1].plot(val_acc)\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52SDt7MvumUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(val_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaiRGCj7MLJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %load_ext tensorboard.notebook\n",
        "# %tensorboard --logdir {logs_base_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ23467psEh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}